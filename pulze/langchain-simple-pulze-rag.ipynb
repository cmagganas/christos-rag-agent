{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"PULZE_API_KEY\")\n",
    "openai.api_base = \"https://api.pulze.ai/v1\"\n",
    "\n",
    "text_response = openai.Completion.create(\n",
    "  model=\"pulze-v0\",\n",
    "  prompt=\"Say Hello World!\",\n",
    ")\n",
    "\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "  model=\"pulze-v0\",\n",
    "  messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Say Hello World!\"\n",
    "  }],\n",
    ")\n",
    "\n",
    "print(text_response, chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Pulze.ai\\n home page\\nSearch...\\nTry for free\\nTry for free\\nSearch\\nNavigation\\nDocumentation\\nTry for free\\nOVERVIEW\\nIntroductions\\nGetting Started\\nModels\\nFEATURES\\nLLMOps\\nKey Features\\nCustom headers\\nLabeling\\nMonitoring\\nLogging\\nRating\\nPlayground\\nTUTORIALS\\nExample app\\nAPI REFERENCE\\nPrompts\\nLogs\\nApps\\nOVERVIEW\\nGetting Started\\n\\u200b\\nWelcome to Pulze.ai\\n\\n\\nPulze.ai is a powerful language model router trained to be an effective tool for various Natural Language Processing (NLP) tasks. Just like OpenAI’s GPT-4 and Anthropic’s Claude, our product is designed for seamless integration with your existing applications, making it more accessible for developers and creative technologists.\\n\\n\\n\\u200b\\nWays to interact with Pulze.ai:', metadata={'source': 'https://docs.pulze.ai/overview/getting-started', 'title': 'Getting Started'}),\n",
       " Document(page_content='Pulze.ai\\n home page\\nSearch...\\nTry for free\\nTry for free\\nSearch\\nNavigation\\nDocumentation\\nTry for free\\nOVERVIEW\\nIntroductions\\nGetting Started\\nModels\\nFEATURES\\nLLMOps\\nKey Features\\nCustom headers\\nLabeling\\nMonitoring\\nLogging\\nRating\\nPlayground\\nTUTORIALS\\nExample app\\nAPI REFERENCE\\nPrompts\\nLogs\\nApps\\nFEATURES\\nPlayground\\nYou can use the playground here\\n\\n\\n\\u200b\\nWhat is it?\\n\\n\\nThe playground is a tool built to showcase the capabilities of Pulze, especially its routing and model selection features.\\n\\n\\nWhen accessing the playground and performing a request, three parallel calls will be done to the top 3 models chosen by\\nour Decision Engine. You can use it to compare how different queries lead to different results.\\n\\n\\n\\u200b\\nHow to use it?', metadata={'source': 'https://docs.pulze.ai/features/playground', 'title': 'Playground'}),\n",
       " Document(page_content='\\u200b\\nWays to interact with Pulze.ai:\\n\\n\\n\\n\\nPulze Playground\\n - Our playground allows you to interact with our language model, experiment with different prompts, and see the response in real time. It’s free for research purposes and available under \\nhttps://pulze.ai/playground\\n.\\n\\n\\nAPI Integration\\n - Integrate Pulze.ai with your product using our API, providing your customers with access to the most powerful large language models (LLMs) available.\\n\\n\\n\\n\\n\\u200b\\nGetting Started\\n\\n\\nSetting up Pulze.ai is simple, free, and should take less than a minute. \\n\\n\\nClick here\\n to sign up.\\n\\n\\n\\u200b\\nPulze.ai Playground\\n\\n\\nWe’ve created a playground environment where you can experiment with different settings, optimizing for speed, cost and quality. Get a feel for the capabilities of our model routing layer. \\nClick here\\n to try it out!\\n\\n\\n\\u200b\\nAPI Integration', metadata={'source': 'https://docs.pulze.ai/overview/getting-started', 'title': 'Getting Started'}),\n",
       " Document(page_content='Pulze.ai\\n home page\\nSearch...\\nTry for free\\nTry for free\\nSearch\\nNavigation\\nDocumentation\\nTry for free\\nOVERVIEW\\nIntroductions\\nGetting Started\\nModels\\nFEATURES\\nLLMOps\\nKey Features\\nCustom headers\\nLabeling\\nMonitoring\\nLogging\\nRating\\nPlayground\\nTUTORIALS\\nExample app\\nAPI REFERENCE\\nPrompts\\nLogs\\nApps\\nFEATURES\\nRating\\nDocumentation will be available soon!\\n\\n\\nIn the meantime, have a look at other pages or give it a go at\\n\\nPulze.ai\\nWas this page helpful?\\nYes\\nNo\\nLogging\\nPlayground\\nwebsite\\ntwitter\\nPowered by Mintlify', metadata={'source': 'https://docs.pulze.ai/features/rating', 'title': 'Rating'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = 'https://api.openai.com/v1' # default\n",
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "embedding_model = OpenAIEmbeddings(model=embedding_model_name)\n",
    "read_vector_store = Chroma(\n",
    "        persist_directory=\"langchain-chroma-pulze-docs\", embedding_function=embedding_model\n",
    "    )\n",
    "query_results = read_vector_store.similarity_search(\"How do I use Pulze?\")\n",
    "query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulze.ai\n",
      " home page\n",
      "Search...\n",
      "Try for free\n",
      "Try for free\n",
      "Search\n",
      "Navigation\n",
      "Documentation\n",
      "Try for free\n",
      "OVERVIEW\n",
      "Introductions\n",
      "Getting Started\n",
      "Models\n",
      "FEATURES\n",
      "LLMOps\n",
      "Key Features\n",
      "Custom headers\n",
      "Labeling\n",
      "Monitoring\n",
      "Logging\n",
      "Rating\n",
      "Playground\n",
      "TUTORIALS\n",
      "Example app\n",
      "API REFERENCE\n",
      "Prompts\n",
      "Logs\n",
      "Apps\n",
      "OVERVIEW\n",
      "Getting Started\n",
      "​\n",
      "Welcome to Pulze.ai\n",
      "\n",
      "\n",
      "Pulze.ai is a powerful language model router trained to be an effective tool for various Natural Language Processing (NLP) tasks. Just like OpenAI’s GPT-4 and Anthropic’s Claude, our product is designed for seamless integration with your existing applications, making it more accessible for developers and creative technologists.\n",
      "\n",
      "\n",
      "​\n",
      "Ways to interact with Pulze.ai:\n"
     ]
    }
   ],
   "source": [
    "print(query_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?')],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    read_vector_store.as_retriever(), \n",
    "    \"search_pulze_docs\",\n",
    "    \"Searches and returns documents regarding Pulze.\"\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature = 0)\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_pulze_docs` with `Pulze LLM Ops best practices`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='Pulze.ai\\n home page\\nSearch...\\nTry for free\\nTry for free\\nSearch\\nNavigation\\nDocumentation\\nTry for free\\nOVERVIEW\\nIntroductions\\nGetting Started\\nModels\\nFEATURES\\nLLMOps\\nKey Features\\nCustom headers\\nLabeling\\nMonitoring\\nLogging\\nRating\\nPlayground\\nTUTORIALS\\nExample app\\nAPI REFERENCE\\nPrompts\\nLogs\\nApps\\nFEATURES\\nLLMOps\\n\\u200b\\nPulze.ai: The Ultimate Solution for Large Language Model Operations (LLMOps)\\n\\n\\n\\u200b\\nIntroduction\\n\\n\\nAs software development evolved over the years, we have seen a transformation from DevOps to MLOps and now the new era of Large Language Model Operations (LLMOps).', metadata={'source': 'https://docs.pulze.ai/features/LLMOps', 'title': 'LLMOps'}), Document(page_content='DevOps emerged as a methodology for ensuring better collaboration and productivity between development and operations teams. The focus of MLOps (Machine Learning Operations) was to accelerate the development, deployment, and monitoring of machine learning models. Now, with the rise of large language models (LLMs) like GPT-4 and Claude, we’re entering the realm of LLMOps.\\n\\n\\nLLMOps is about operationalizing large language models in applications to power a myriad of tasks such as text generation, prompt execution, real-time language translation, and more. In this complex and rapidly evolving field, one name stands out: Pulze.ai.\\n\\n\\nIn this document, we delve into why Pulze.ai is the ideal solution for anyone requiring expertise in LLMOps, providing insights into its features and how it empowers users.\\n\\n\\n\\u200b\\nKey Features of Pulze.ai', metadata={'source': 'https://docs.pulze.ai/features/LLMOps', 'title': 'LLMOps'}), Document(page_content='\\u200b\\nConclusion\\n\\n\\nIn the landscape of LLMOps, Pulze.ai emerges as a comprehensive and robust platform that facilitates seamless management, optimization, and control of large language models. The platform’s features make it an ideal choice for developers and enterprises seeking to harness the power of large language models in an efficient, scalable, and enterprise-ready manner.\\nWas this page helpful?\\nYes\\nNo\\nModels\\nKey Features\\nwebsite\\ntwitter\\nPowered by Mintlify\\nPulze.ai: The Ultimate Solution for Large Language Model Operations (LLMOps)\\nIntroduction\\nKey Features of Pulze.ai\\nApp Hosting and Orchestration\\nValidation and Benchmarking\\nLLM Caching\\nLLM Logging and Observability\\nIntegrations\\nEnterprise-Ready Scalability\\nConclusion', metadata={'source': 'https://docs.pulze.ai/features/LLMOps', 'title': 'LLMOps'}), Document(page_content='\\u200b\\nLLM Caching\\n\\n\\nPulze.ai implements advanced LLM caching techniques to optimize model performance and reduce latency. This is crucial for applications that require real-time or near real-time responses.\\n\\n\\n\\u200b\\nLLM Logging and Observability\\n\\n\\nPulze.ai provides comprehensive logging and observability features, allowing you to monitor all model requests and performance metrics. With transparent response conversions and detailed metrics, you can gain critical insights into model performance and make data-driven decisions.\\n\\n\\n\\u200b\\nIntegrations\\n\\n\\nThe platform offers seamless integration with both proprietary APIs and open APIs. This flexibility allows developers to leverage a wide variety of large language models, including the latest open-source models and custom models specific to their application needs.\\n\\n\\n\\u200b\\nEnterprise-Ready Scalability', metadata={'source': 'https://docs.pulze.ai/features/LLMOps', 'title': 'LLMOps'})]\u001b[0m\u001b[32;1m\u001b[1;3mTo use Pulze with LLM Ops best practices in mind, you can leverage the following key features of Pulze.ai:\n",
      "\n",
      "1. App Hosting and Orchestration: Pulze.ai provides a platform for hosting and orchestrating your applications that utilize large language models. It allows you to deploy and manage your applications efficiently.\n",
      "\n",
      "2. Validation and Benchmarking: Pulze.ai offers validation and benchmarking capabilities to ensure the performance and accuracy of your large language models. You can validate your models against various metrics and benchmarks to ensure their quality.\n",
      "\n",
      "3. LLM Caching: Pulze.ai implements advanced LLM caching techniques to optimize model performance and reduce latency. This is particularly important for applications that require real-time or near real-time responses.\n",
      "\n",
      "4. LLM Logging and Observability: Pulze.ai provides comprehensive logging and observability features. You can monitor all model requests and performance metrics, gain insights into model performance, and make data-driven decisions based on transparent response conversions and detailed metrics.\n",
      "\n",
      "5. Integrations: Pulze.ai offers seamless integration with both proprietary APIs and open APIs. This flexibility allows developers to leverage a wide variety of large language models, including the latest open-source models and custom models specific to their application needs.\n",
      "\n",
      "6. Enterprise-Ready Scalability: Pulze.ai is designed to be scalable and enterprise-ready. It can handle the demands of large-scale applications and ensure the efficient management and control of large language models.\n",
      "\n",
      "By utilizing these features, Pulze.ai enables you to effectively operationalize large language models in your applications, empowering you to perform tasks such as text generation, prompt execution, real-time language translation, and more in a reliable and efficient manner.\n",
      "\n",
      "For more detailed information, you can refer to the [Pulze.ai LLMOps documentation](https://docs.pulze.ai/features/LLMOps).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To use Pulze with LLM Ops best practices in mind, you can leverage the following key features of Pulze.ai:\\n\\n1. App Hosting and Orchestration: Pulze.ai provides a platform for hosting and orchestrating your applications that utilize large language models. It allows you to deploy and manage your applications efficiently.\\n\\n2. Validation and Benchmarking: Pulze.ai offers validation and benchmarking capabilities to ensure the performance and accuracy of your large language models. You can validate your models against various metrics and benchmarks to ensure their quality.\\n\\n3. LLM Caching: Pulze.ai implements advanced LLM caching techniques to optimize model performance and reduce latency. This is particularly important for applications that require real-time or near real-time responses.\\n\\n4. LLM Logging and Observability: Pulze.ai provides comprehensive logging and observability features. You can monitor all model requests and performance metrics, gain insights into model performance, and make data-driven decisions based on transparent response conversions and detailed metrics.\\n\\n5. Integrations: Pulze.ai offers seamless integration with both proprietary APIs and open APIs. This flexibility allows developers to leverage a wide variety of large language models, including the latest open-source models and custom models specific to their application needs.\\n\\n6. Enterprise-Ready Scalability: Pulze.ai is designed to be scalable and enterprise-ready. It can handle the demands of large-scale applications and ensure the efficient management and control of large language models.\\n\\nBy utilizing these features, Pulze.ai enables you to effectively operationalize large language models in your applications, empowering you to perform tasks such as text generation, prompt execution, real-time language translation, and more in a reliable and efficient manner.\\n\\nFor more detailed information, you can refer to the [Pulze.ai LLMOps documentation](https://docs.pulze.ai/features/LLMOps).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent_executor({\"input\": \"How do I use Pulze with LLM Ops best practices in mind?\"})\n",
    "result[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Pulze with LLM Ops best practices in mind, you can leverage the following key features of Pulze.ai:\n",
      "\n",
      "1. App Hosting and Orchestration: Pulze.ai provides a platform for hosting and orchestrating your applications that utilize large language models. It allows you to deploy and manage your applications efficiently.\n",
      "\n",
      "2. Validation and Benchmarking: Pulze.ai offers validation and benchmarking capabilities to ensure the performance and accuracy of your large language models. You can validate your models against various metrics and benchmarks to ensure their quality.\n",
      "\n",
      "3. LLM Caching: Pulze.ai implements advanced LLM caching techniques to optimize model performance and reduce latency. This is particularly important for applications that require real-time or near real-time responses.\n",
      "\n",
      "4. LLM Logging and Observability: Pulze.ai provides comprehensive logging and observability features. You can monitor all model requests and performance metrics, gain insights into model performance, and make data-driven decisions based on transparent response conversions and detailed metrics.\n",
      "\n",
      "5. Integrations: Pulze.ai offers seamless integration with both proprietary APIs and open APIs. This flexibility allows developers to leverage a wide variety of large language models, including the latest open-source models and custom models specific to their application needs.\n",
      "\n",
      "6. Enterprise-Ready Scalability: Pulze.ai is designed to be scalable and enterprise-ready. It can handle the demands of large-scale applications and ensure the efficient management and control of large language models.\n",
      "\n",
      "By utilizing these features, Pulze.ai enables you to effectively operationalize large language models in your applications, empowering you to perform tasks such as text generation, prompt execution, real-time language translation, and more in a reliable and efficient manner.\n",
      "\n",
      "For more detailed information, you can refer to the [Pulze.ai LLMOps documentation](https://docs.pulze.ai/features/LLMOps).\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim-llm-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
